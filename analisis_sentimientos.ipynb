{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimientos\n",
    "\n",
    "__Qué necesitarás?__\n",
    "\n",
    "* pandas : librería de manipulación y análisis de datos de código abierto rápida, potente, flexible y fácil de usar, construida sobre el lenguaje de programación Python\n",
    "* numpy : paquete fundamental para la computación científica en Python. Es una biblioteca de Python que proporciona un objeto de matriz multidimensional, varios objetos derivados (como matrices y matrices enmascaradas) y una variedad de rutinas para operaciones rápidas en matrices, que incluyen manipulación matemática, lógica, clasificación, selección, transformadas discretas de Fourier, álgebra lineal básica, operaciones estadísticas básicas, simulación aleatoria ...\n",
    "* train_test_split: función de sklearn.model_selection que permite particionar la data de entrenamiento y testeo para fines de entrenamiento en modelos de ML e IA\n",
    "* test_size: patron para muestra de prueba. El dato se define entre 0 y 1\n",
    "* random_state: semilla para generador de números aleatorios, lo que permite reporducir la función y garantizar la constancia del resultado obtenido\n",
    "* CountVectorizer : función de la librería de sklearn.feature_extraction.text, la cual permite la extracción de características de texto ---> https://qu4nt.github.io/sklearn-doc-es/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "* token_pattern : Expresión regular que denota lo que constituye un «token», sólo se utiliza si analyzer ---> {“word”, “char”, “char_wb”} o callable, default=”word”\n",
    "* lowercase : Convierte todos los caracteres en minúsculas antes de la tokenización\n",
    "* PorterStemmer : función de nltk.stem.porter la cual permite eliminar los afijos morfológicos de las palabras, dejando solo la raíz de la palabra ----> https://www.nltk.org/howto/stem.html\n",
    "* build_analyzer : Devuelve un invocable para procesar los datos de entrada. Los identificadores invocables que manejan el preprocesamiento, la tokenización y la generación de n-gramas (subsecuencia de n elementos de una secuencia dada).\n",
    "* GridSearchCV : función de sklearn.model_selection conocida como instancia que permite realizar busqueda exhaustiva sobre valores de parámetros específicos para un estimador ---> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html \n",
    "* estimator (GridSearchCV) : implementa la interfaz del estimador scikit-learn\n",
    "* param_grid (GridSearchCV) : Diccionario con nombres de parámetros ( str) como claves y listas de configuraciones de parámetros para probar como valores, o una lista de dichos diccionarios, en cuyo caso se exploran las cuadrículas que abarca cada diccionario de la lista. Esto permite buscar en cualquier secuencia de ajustes de parámetros.\n",
    "* cv (GridSearchCV) : Generador de validación cruzada. Determina la estrategia de división de validación cruzada. Algunas de las entradas posibles para cv son: Ninguno, para usar la validación cruzada de 5 veces predeterminada; entero, para especificar el número de pliegues.\n",
    "* scoring (GridSearchCV) : Estrategia para evaluar el rendimiento del modelo de validación cruzada en el conjunto de prueba.\n",
    "* refit (GridSearchCV) :  booleano que realiza o no ajuste de un estimador utilizando los mejores parámetros encontrados en todo el conjunto de datos\n",
    "* return_train_score (GridSearchCV) : booleano que devuelve puntajes de entrenamiento\n",
    "* Pipeline : función de sklearn.pipeline que actua como canalizador para ensamblar varios pasos que se pueden validar de forma cruzada mientras se configuran diferentes parámetros ----> https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html \n",
    "* BernoulliNB : función de sklearn.naive_bayes conocido como clasificador Naive Bayes para modelos multivariados de Bernoulli. BernoulliNB está diseñado para características binarias/booleanas ----> https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html \n",
    "* confusion_matrix : función de sklearn.metrics que permite evaluar la precisión de una clasificación.\n",
    "* variable y: variable dependiente cuyos valores dependen de los que tomen otra(s) variable(s)\n",
    "* Variable x: Variable independiente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso de uso\n",
    "\n",
    "El archivo `data.txt` contiene una serie de comentarios sobre productos\n",
    "de la tienda de amazon, los cuales están etiquetados como positivos (1), negativos (0)\n",
    "o indterminados (NULL).\n",
    "\n",
    "Haciendo uso de Naive Bayes, realice análisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos a Seguir\n",
    "\n",
    "1. Cargue los datos haciendo uso de la librería de pandas, indicando que la columna 0 corresponde al mensaje y la columna 1 corresponde a su etiqueta.\n",
    "2. Obtenga los grupos de mensajes (etiquetados y sin etiquetar), definiendo como variable independependiente el mensaje y como dependiente la etiqueta\n",
    "3. Preparar conjunto de datos: Partione la data, de tal manera que obtenga la data de entrenamiento para variable independiente y dependiente, a partir de los grupos de datos etiquetados.Es de resaltar que, la semilla del generador de números aleatorios debe ser 12345 y debe usar el 10% de patrones para la muestra de prueba\n",
    "4. Cunstruya un analizador de palabras: crear _stemmer_, compilar un _vectorizer_, crear instancia a partir del _vectorizer_ haciendo uso de build_analyzer, ejecutar el analizador\n",
    "5. Crear una instancia de CountVectorizer que use el analizador de palabras antes mencionado: Esta instancia debe retornar una matriz binaria. El límite superior para la frecuencia de palabras es del 100% y un límite inferior de 5 palabras. Solo deben analizarse palabras conformadas por letras.\n",
    "6. Crear pipeline que contenga el CountVectorizer mencionado en el ítem anterior y el modelo de BernoulliNB, con el objetivo de definir el estimador de GridSearchCV previo al entrenamiento.\n",
    "7. Definir un diccionario de parámetros para el GridSearchCV (param_grid). Se deben considerar 10 valores entre 0.1 y 1.0 para el parámetro alpha de BernoulliNB.\n",
    "8. Definir una instancia de GridSearchCV con el pipeline y el diccionario de parámetros. Use cv=5, y \"accuracy\" como métrica de evaluación\n",
    "9. Buscar la mejor combinación de regresores: entrenar instancia de GridSearchCV\n",
    "10. Evaluar el modelo con los datos de entrenamiento y de prueba usando la matriz de confusión de sklearn.metrics\n",
    "11. Pronosticar la polaridad del sentimiento para los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def read_data(input, columns_data):\n",
    "    df = pd.read_csv(\n",
    "        input, \n",
    "        sep=\"\\t\", \n",
    "        header=None, \n",
    "        names=columns_data\n",
    "    )\n",
    "    return df\n",
    "\n",
    "#2\n",
    "\n",
    "def group_data_for_label(df):\n",
    "    df_tagged = df[(df[\"etiqueta\"]>=0)]\n",
    "    df_untagged = df[(df[\"etiqueta\"].isna())]\n",
    "\n",
    "    x_tagged = df_tagged[\"mensaje\"]\n",
    "    y_tagged = df_tagged[\"etiqueta\"]\n",
    "\n",
    "    x_untagged = df_untagged[\"mensaje\"]\n",
    "    y_untagged = df_untagged[\"etiqueta\"]\n",
    "\n",
    "    return x_tagged, y_tagged, x_untagged, y_untagged\n",
    "\n",
    "#3\n",
    "\n",
    "def data_set_preparation(x_tagged, y_tagged):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split( \n",
    "        x_tagged, \n",
    "        y_tagged,\n",
    "        test_size=0.1, \n",
    "        random_state=12345\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "#4\n",
    "def building_word_analyzer():\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    #________________________________\n",
    "    vectorizer= CountVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\", \n",
    "        lowercase=True\n",
    "    )\n",
    "    #_________________________________\n",
    "    analyzer =vectorizer.build_analyzer()\n",
    "\n",
    "    return lambda x: (stemmer.stem(w) for w in analyzer(x))\n",
    "\n",
    "# 5\n",
    "\n",
    "def instance_countvectorizer(analyzer):\n",
    "    countVectorizer = CountVectorizer(\n",
    "        analyzer=analyzer,\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\", \n",
    "        binary=True, \n",
    "        max_df=1.0, \n",
    "        min_df=5\n",
    "    )\n",
    "    return countVectorizer\n",
    "\n",
    "#6\n",
    "\n",
    "def create_pipeline(countVectorizer):\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"countVectorizer\", countVectorizer),\n",
    "            (\"bernoulli\", BernoulliNB()),\n",
    "        ],\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "#7 y 8\n",
    "\n",
    "def create_instance_gridsearchCV(pipeline, x_train, y_train):\n",
    "    #7__________________________________\n",
    "    param_grid = {\n",
    "        \"bernoulli__alpha\": np.arange(0.1,1.01, 0.1),\n",
    "    }\n",
    "    #8_________________________________\n",
    "    gridSearchCV = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid= param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        refit=True,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    #9________________________________\n",
    "    gridSearchCV.fit(x_train, y_train)\n",
    "\n",
    "    return gridSearchCV\n",
    "\n",
    "#10\n",
    "def assess_model(gridSearchCV,x_train, x_test, y_train, y_test ):\n",
    "\n",
    "    #trein____________________________________________\n",
    "    cfm_train = confusion_matrix(\n",
    "        y_true=y_train,\n",
    "        y_pred=gridSearchCV.predict(x_train),\n",
    "    )\n",
    "\n",
    "    #test______________________________________________\n",
    "    cfm_test = confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=gridSearchCV.predict(x_test),\n",
    "    )\n",
    "    return cfm_train, cfm_test\n",
    "\n",
    "#11\n",
    "def predict_tags(gridSearchCV, x_untagged):\n",
    "\n",
    "    y_untagged_pred = gridSearchCV.predict(x_untagged)\n",
    "\n",
    "    return y_untagged_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"data.txt\"\n",
    "columns_data=[\"mensaje\",\"etiqueta\"]\n",
    "data= read_data(input, columns_data)\n",
    "x_tagged, y_tagged, x_untagged, y_untagged = group_data_for_label(data)\n",
    "x_train, x_test, y_train, y_test = data_set_preparation(x_tagged, y_tagged)\n",
    "analyzer=building_word_analyzer()\n",
    "countVectorizer= instance_countvectorizer(analyzer)\n",
    "pipeline = create_pipeline(countVectorizer)\n",
    "gridSearchCV = create_instance_gridsearchCV(pipeline, x_train, y_train)\n",
    "cfm_train, cfm_test = assess_model(gridSearchCV,x_train, x_test, y_train, y_test )\n",
    "y_untagged_pred= predict_tags(gridSearchCV, x_untagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13609\n",
      "13609\n"
     ]
    }
   ],
   "source": [
    "print(len(y_untagged_pred))\n",
    "print(len(x_untagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96724d6b9637267567451acd5db668b6a362e9d39d8edb46a4de17ef5590deaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
